---
layout: archive

permalink: /thoughts/attention
author_profile: true
redirect_from:
  - /attention
---


{% include base_path %}


Since its debut in Neural Machine Translation(NMT), attention mechanism has been witnessed widely and frequently in various studies of Natural Language Processing(NLP) and Computer Vision(CV). As I am unfamiliar with CV, here I will only talk a bit about what attention is by intuition and in implementation, how it works and hopefully why it proves useful.

The editor converts LaTeX equations in double-dollars <code>$$</code>: <img src="//tex.s2cms.ru/svg/ax%5E2%2Bbx%2Bc%3D0" alt="ax^2+bx+c=0" />. All equations are rendered as block equations. If you need inline ones, you can add the prefix <code>\inline</code>: <img src="//tex.s2cms.ru/svg/%5Cinline%20p%3D%7B1%5Cover%20q%7D" alt="\inline p={1\over q}" />. But it is a good practice to place big equations on separate lines: